{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b20b488d-fa79-44fe-8371-e3304165f008",
   "metadata": {},
   "source": [
    "## 1. Load the MNIST Digit dataset, show the size of the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44a30566-ae5d-4a58-a297-62ab7f8f3a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (60000, 28, 28) (images) and (60000,) (labels)\n",
      "Test set shape: (10000, 28, 28) (images) and (10000,) (labels)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data(path=\"mnist.npz\")\n",
    "\n",
    "# Print the shape of the training and testing data\n",
    "print(f\"Training set shape: {X_train.shape} (images) and {y_train.shape} (labels)\")\n",
    "print(f\"Test set shape: {X_test.shape} (images) and {y_test.shape} (labels)\")\n",
    "\n",
    "# Plot a few sample images\n",
    "plt.figure(figsize=(8, 4))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(X_train[i], cmap='gray')\n",
    "    plt.title(f\"Label: {y_train[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90610b76-2212-4f59-863f-b968ec852a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the images and normalize pixel values\n",
    "X_train_flat = X_train.reshape(-1, 28 * 28) / 255.0\n",
    "X_test_flat = X_test.reshape(-1, 28 * 28) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9528efa3-487f-44bf-bd8d-4b843cea09ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_flat.shape,X_test_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e337da2-1bed-4cf7-9486-78109aacc40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('uint8'), dtype('uint8'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.dtype,y_test.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b149df3a-8498-4c64-8e50-c1c124470752",
   "metadata": {},
   "source": [
    "## 2. Develop a one hidden layer multi-layer perceptron model on the above training data, report the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1897de29-2f8a-40ee-a4f5-787841effe7b",
   "metadata": {},
   "source": [
    "#### Create the MLP Model: We’ll use Keras to define our one-hidden-layer MLP. Here’s a simple architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a99ca43c-8d24-4c2c-8f06-182a1f7e887c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=128, activation='relu', input_shape=(784,)))  # Hidden layer with 128 units\n",
    "model.add(Dense(units=10, activation='softmax'))  # Output layer (10 classes for digits 0 to 9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a1f7cf-eba9-4be3-98ca-a0278d379397",
   "metadata": {},
   "source": [
    "#### Compile the Model: Specify the loss function, optimizer, and evaluation metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9186f8be-107e-4fe1-a877-1cb541a68e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d41a9fc-b67b-4864-ad5c-512439a82977",
   "metadata": {},
   "source": [
    "#### Train the Model: Let’s train our model using the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77eb3592-cd88-4141-a220-11576c4417f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8646 - loss: 0.4716 - val_accuracy: 0.9519 - val_loss: 0.1655\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9602 - loss: 0.1363 - val_accuracy: 0.9650 - val_loss: 0.1196\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9748 - loss: 0.0886 - val_accuracy: 0.9696 - val_loss: 0.1024\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9801 - loss: 0.0656 - val_accuracy: 0.9700 - val_loss: 0.1018\n",
      "Epoch 5/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9851 - loss: 0.0492 - val_accuracy: 0.9744 - val_loss: 0.0898\n",
      "Epoch 6/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9883 - loss: 0.0383 - val_accuracy: 0.9758 - val_loss: 0.0822\n",
      "Epoch 7/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9917 - loss: 0.0297 - val_accuracy: 0.9762 - val_loss: 0.0873\n",
      "Epoch 8/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9931 - loss: 0.0238 - val_accuracy: 0.9775 - val_loss: 0.0858\n",
      "Epoch 9/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9956 - loss: 0.0173 - val_accuracy: 0.9756 - val_loss: 0.0975\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9957 - loss: 0.0154 - val_accuracy: 0.9747 - val_loss: 0.0980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1ec1fe53e90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_flat, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7050c059-4377-4d20-bc75-5673272068f0",
   "metadata": {},
   "source": [
    "#### Evaluate Accuracy on Test Set: Finally, let’s see how well our model performs on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c73583e-e7be-4180-87ab-f98621764468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 684us/step - accuracy: 0.9955 - loss: 0.0169\n",
      "Train accuracy: 0.9913\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_accuracy = model.evaluate(X_train_flat, y_train)\n",
    "print(f\"Train accuracy: {train_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c8c3b32-2d34-4a1a-a406-df387617ad6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 887us/step - accuracy: 0.9740 - loss: 0.1018\n",
      "Test accuracy: 0.9774\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test_flat, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6d8873-a477-4188-9d67-48732f3f2e32",
   "metadata": {},
   "source": [
    "##  3. From question 2, set the number of hidden layers of the MLP model as [2,4,6,8,10], set the hidden layer size as 100, show the accuracies on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7abdc505-a54e-4d67-aed2-bf707783405c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8630 - loss: 0.4755 - val_accuracy: 0.9575 - val_loss: 0.1417\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9635 - loss: 0.1208 - val_accuracy: 0.9622 - val_loss: 0.1265\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9778 - loss: 0.0727 - val_accuracy: 0.9673 - val_loss: 0.1055\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9810 - loss: 0.0592 - val_accuracy: 0.9695 - val_loss: 0.1002\n",
      "Epoch 5/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9870 - loss: 0.0423 - val_accuracy: 0.9744 - val_loss: 0.0885\n",
      "Epoch 6/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9892 - loss: 0.0343 - val_accuracy: 0.9708 - val_loss: 0.1056\n",
      "Epoch 7/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9904 - loss: 0.0299 - val_accuracy: 0.9735 - val_loss: 0.0978\n",
      "Epoch 8/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9923 - loss: 0.0228 - val_accuracy: 0.9751 - val_loss: 0.1028\n",
      "Epoch 9/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9928 - loss: 0.0206 - val_accuracy: 0.9728 - val_loss: 0.1189\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9938 - loss: 0.0174 - val_accuracy: 0.9724 - val_loss: 0.1183\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708us/step - accuracy: 0.9929 - loss: 0.0217\n",
      "Hidden Layers: 2 | Train accuracy: 0.9891\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - accuracy: 0.9708 - loss: 0.1275\n",
      "Hidden Layers: 2 | Test accuracy: 0.9751\n",
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8510 - loss: 0.4834 - val_accuracy: 0.9518 - val_loss: 0.1493\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9637 - loss: 0.1162 - val_accuracy: 0.9678 - val_loss: 0.1089\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9725 - loss: 0.0861 - val_accuracy: 0.9668 - val_loss: 0.1085\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9807 - loss: 0.0637 - val_accuracy: 0.9643 - val_loss: 0.1189\n",
      "Epoch 5/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9843 - loss: 0.0528 - val_accuracy: 0.9709 - val_loss: 0.0999\n",
      "Epoch 6/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9857 - loss: 0.0445 - val_accuracy: 0.9710 - val_loss: 0.1150\n",
      "Epoch 7/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9874 - loss: 0.0398 - val_accuracy: 0.9712 - val_loss: 0.1103\n",
      "Epoch 8/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9878 - loss: 0.0362 - val_accuracy: 0.9749 - val_loss: 0.0977\n",
      "Epoch 9/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9899 - loss: 0.0318 - val_accuracy: 0.9720 - val_loss: 0.1136\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9922 - loss: 0.0249 - val_accuracy: 0.9692 - val_loss: 0.1405\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 849us/step - accuracy: 0.9871 - loss: 0.0405\n",
      "Hidden Layers: 4 | Train accuracy: 0.9837\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - accuracy: 0.9653 - loss: 0.1429\n",
      "Hidden Layers: 4 | Test accuracy: 0.9697\n",
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8269 - loss: 0.5350 - val_accuracy: 0.9560 - val_loss: 0.1484\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9586 - loss: 0.1346 - val_accuracy: 0.9672 - val_loss: 0.1134\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9698 - loss: 0.1025 - val_accuracy: 0.9610 - val_loss: 0.1423\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9745 - loss: 0.0840 - val_accuracy: 0.9688 - val_loss: 0.1162\n",
      "Epoch 5/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9800 - loss: 0.0682 - val_accuracy: 0.9699 - val_loss: 0.1141\n",
      "Epoch 6/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9817 - loss: 0.0613 - val_accuracy: 0.9734 - val_loss: 0.1043\n",
      "Epoch 7/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9852 - loss: 0.0513 - val_accuracy: 0.9732 - val_loss: 0.1131\n",
      "Epoch 8/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9873 - loss: 0.0451 - val_accuracy: 0.9745 - val_loss: 0.1064\n",
      "Epoch 9/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9886 - loss: 0.0366 - val_accuracy: 0.9765 - val_loss: 0.1036\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9899 - loss: 0.0353 - val_accuracy: 0.9762 - val_loss: 0.1094\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 807us/step - accuracy: 0.9938 - loss: 0.0208\n",
      "Hidden Layers: 6 | Train accuracy: 0.9906\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 803us/step - accuracy: 0.9745 - loss: 0.1173\n",
      "Hidden Layers: 6 | Test accuracy: 0.9774\n",
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.8014 - loss: 0.6031 - val_accuracy: 0.9421 - val_loss: 0.2072\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9532 - loss: 0.1626 - val_accuracy: 0.9458 - val_loss: 0.1967\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9666 - loss: 0.1209 - val_accuracy: 0.9617 - val_loss: 0.1405\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9714 - loss: 0.0976 - val_accuracy: 0.9700 - val_loss: 0.1116\n",
      "Epoch 5/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9766 - loss: 0.0832 - val_accuracy: 0.9698 - val_loss: 0.1154\n",
      "Epoch 6/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9815 - loss: 0.0679 - val_accuracy: 0.9658 - val_loss: 0.1217\n",
      "Epoch 7/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9831 - loss: 0.0617 - val_accuracy: 0.9714 - val_loss: 0.1108\n",
      "Epoch 8/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9847 - loss: 0.0548 - val_accuracy: 0.9678 - val_loss: 0.1303\n",
      "Epoch 9/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9864 - loss: 0.0473 - val_accuracy: 0.9744 - val_loss: 0.1075\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9879 - loss: 0.0442 - val_accuracy: 0.9740 - val_loss: 0.1165\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9917 - loss: 0.0297\n",
      "Hidden Layers: 8 | Train accuracy: 0.9886\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9728 - loss: 0.1225\n",
      "Hidden Layers: 8 | Test accuracy: 0.9781\n",
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7821 - loss: 0.6442 - val_accuracy: 0.9456 - val_loss: 0.1933\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9549 - loss: 0.1680 - val_accuracy: 0.9595 - val_loss: 0.1642\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9661 - loss: 0.1294 - val_accuracy: 0.9637 - val_loss: 0.1385\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9722 - loss: 0.1027 - val_accuracy: 0.9603 - val_loss: 0.1456\n",
      "Epoch 5/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9766 - loss: 0.0878 - val_accuracy: 0.9672 - val_loss: 0.1474\n",
      "Epoch 6/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9785 - loss: 0.0807 - val_accuracy: 0.9688 - val_loss: 0.1180\n",
      "Epoch 7/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9826 - loss: 0.0634 - val_accuracy: 0.9707 - val_loss: 0.1444\n",
      "Epoch 8/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9827 - loss: 0.0665 - val_accuracy: 0.9656 - val_loss: 0.1372\n",
      "Epoch 9/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9858 - loss: 0.0553 - val_accuracy: 0.9647 - val_loss: 0.1735\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9844 - loss: 0.0576 - val_accuracy: 0.9597 - val_loss: 0.1536\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9749 - loss: 0.0778\n",
      "Hidden Layers: 10 | Train accuracy: 0.9722\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9539 - loss: 0.1754\n",
      "Hidden Layers: 10 | Test accuracy: 0.9589\n"
     ]
    }
   ],
   "source": [
    "# Define a function to create an MLP model\n",
    "def create_mlp(hidden_layers):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=100, activation='relu', input_shape=(784,)))  # First hidden layer\n",
    "    for _ in range(hidden_layers - 1):\n",
    "        model.add(Dense(units=100, activation='relu'))  # Additional hidden layers\n",
    "    model.add(Dense(units=10, activation='softmax'))  # Output layer (10 classes)\n",
    "    return model\n",
    "\n",
    "# Evaluate models with different numbers of hidden layers\n",
    "hidden_layer_sizes = [2, 4, 6, 8, 10]\n",
    "for num_layers in hidden_layer_sizes:\n",
    "    mlp_model = create_mlp(hidden_layers=num_layers)\n",
    "    mlp_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    mlp_model.fit(X_train_flat, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "    train_loss, train_accuracy = mlp_model.evaluate(X_train_flat, y_train)\n",
    "    print(f\"Hidden Layers: {num_layers} | Train accuracy: {train_accuracy:.4f}\")\n",
    "    test_loss, test_accuracy = mlp_model.evaluate(X_test_flat, y_test)\n",
    "    print(f\"Hidden Layers: {num_layers} | Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d49dc5-78b7-488b-9f00-22f9871e3c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f79df6ba-469a-47ba-858e-5bcb88e5cd9b",
   "metadata": {},
   "source": [
    "## 4. From question 2, for the hidden layer set the hidden layer size as [50, 100, 150, 200], show the accuracies on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cabc9a0-7e8c-429e-aacc-bf60ff3a096b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.8348 - loss: 0.5707 - val_accuracy: 0.9453 - val_loss: 0.1968\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9429 - loss: 0.1917 - val_accuracy: 0.9554 - val_loss: 0.1588\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9588 - loss: 0.1376 - val_accuracy: 0.9613 - val_loss: 0.1356\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9682 - loss: 0.1110 - val_accuracy: 0.9629 - val_loss: 0.1303\n",
      "Epoch 5/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9732 - loss: 0.0893 - val_accuracy: 0.9647 - val_loss: 0.1223\n",
      "Epoch 6/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9768 - loss: 0.0768 - val_accuracy: 0.9668 - val_loss: 0.1127\n",
      "Epoch 7/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9809 - loss: 0.0658 - val_accuracy: 0.9672 - val_loss: 0.1116\n",
      "Epoch 8/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9822 - loss: 0.0580 - val_accuracy: 0.9698 - val_loss: 0.1121\n",
      "Epoch 9/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9854 - loss: 0.0500 - val_accuracy: 0.9658 - val_loss: 0.1195\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9860 - loss: 0.0459 - val_accuracy: 0.9673 - val_loss: 0.1172\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 688us/step - accuracy: 0.9882 - loss: 0.0408\n",
      "Hidden Layer Size: 50 | Train accuracy: 0.9837\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - accuracy: 0.9654 - loss: 0.1159\n",
      "Hidden Layer Size: 50 | Test accuracy: 0.9685\n",
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.8581 - loss: 0.4984 - val_accuracy: 0.9531 - val_loss: 0.1652\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9574 - loss: 0.1456 - val_accuracy: 0.9632 - val_loss: 0.1255\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9708 - loss: 0.0990 - val_accuracy: 0.9673 - val_loss: 0.1103\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9789 - loss: 0.0706 - val_accuracy: 0.9709 - val_loss: 0.0971\n",
      "Epoch 5/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9844 - loss: 0.0539 - val_accuracy: 0.9707 - val_loss: 0.0961\n",
      "Epoch 6/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9866 - loss: 0.0442 - val_accuracy: 0.9740 - val_loss: 0.0881\n",
      "Epoch 7/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9907 - loss: 0.0333 - val_accuracy: 0.9742 - val_loss: 0.0915\n",
      "Epoch 8/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9920 - loss: 0.0278 - val_accuracy: 0.9735 - val_loss: 0.0970\n",
      "Epoch 9/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9930 - loss: 0.0250 - val_accuracy: 0.9743 - val_loss: 0.0934\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9951 - loss: 0.0176 - val_accuracy: 0.9729 - val_loss: 0.0984\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 750us/step - accuracy: 0.9943 - loss: 0.0182\n",
      "Hidden Layer Size: 100 | Train accuracy: 0.9905\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - accuracy: 0.9707 - loss: 0.1024\n",
      "Hidden Layer Size: 100 | Test accuracy: 0.9759\n",
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8692 - loss: 0.4629 - val_accuracy: 0.9578 - val_loss: 0.1476\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9638 - loss: 0.1280 - val_accuracy: 0.9689 - val_loss: 0.1063\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9757 - loss: 0.0800 - val_accuracy: 0.9709 - val_loss: 0.0956\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9817 - loss: 0.0621 - val_accuracy: 0.9732 - val_loss: 0.0878\n",
      "Epoch 5/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9860 - loss: 0.0440 - val_accuracy: 0.9737 - val_loss: 0.0900\n",
      "Epoch 6/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9904 - loss: 0.0327 - val_accuracy: 0.9725 - val_loss: 0.0956\n",
      "Epoch 7/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9930 - loss: 0.0240 - val_accuracy: 0.9757 - val_loss: 0.0835\n",
      "Epoch 8/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9940 - loss: 0.0209 - val_accuracy: 0.9747 - val_loss: 0.0962\n",
      "Epoch 9/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9956 - loss: 0.0155 - val_accuracy: 0.9722 - val_loss: 0.1027\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9961 - loss: 0.0135 - val_accuracy: 0.9762 - val_loss: 0.0937\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 789us/step - accuracy: 0.9968 - loss: 0.0118\n",
      "Hidden Layer Size: 150 | Train accuracy: 0.9931\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 892us/step - accuracy: 0.9772 - loss: 0.0893\n",
      "Hidden Layer Size: 150 | Test accuracy: 0.9795\n",
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8760 - loss: 0.4330 - val_accuracy: 0.9619 - val_loss: 0.1325\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9653 - loss: 0.1161 - val_accuracy: 0.9681 - val_loss: 0.1051\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9780 - loss: 0.0728 - val_accuracy: 0.9719 - val_loss: 0.0970\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9855 - loss: 0.0499 - val_accuracy: 0.9722 - val_loss: 0.0930\n",
      "Epoch 5/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9882 - loss: 0.0391 - val_accuracy: 0.9768 - val_loss: 0.0840\n",
      "Epoch 6/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9925 - loss: 0.0268 - val_accuracy: 0.9734 - val_loss: 0.0940\n",
      "Epoch 7/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9939 - loss: 0.0205 - val_accuracy: 0.9730 - val_loss: 0.0959\n",
      "Epoch 8/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9953 - loss: 0.0160 - val_accuracy: 0.9759 - val_loss: 0.0886\n",
      "Epoch 9/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9961 - loss: 0.0137 - val_accuracy: 0.9747 - val_loss: 0.1012\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9968 - loss: 0.0107 - val_accuracy: 0.9763 - val_loss: 0.0954\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9977 - loss: 0.0094\n",
      "Hidden Layer Size: 200 | Train accuracy: 0.9937\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step - accuracy: 0.9756 - loss: 0.0901\n",
      "Hidden Layer Size: 200 | Test accuracy: 0.9793\n"
     ]
    }
   ],
   "source": [
    "# Define a function to create an MLP model with varying hidden layer size\n",
    "def create_mlp(hidden_layer_size):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=hidden_layer_size, activation='relu', input_shape=(784,)))  # Hidden layer\n",
    "    model.add(Dense(units=10, activation='softmax'))  # Output layer (10 classes)\n",
    "    return model\n",
    "\n",
    "# Evaluate models with different hidden layer sizes\n",
    "hidden_layer_sizes = [50, 100, 150, 200]\n",
    "for size in hidden_layer_sizes:\n",
    "    mlp_model = create_mlp(hidden_layer_size=size)\n",
    "    mlp_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    mlp_model.fit(X_train_flat, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "    train_loss, train_accuracy = mlp_model.evaluate(X_train_flat, y_train)\n",
    "    print(f\"Hidden Layer Size: {size} | Train accuracy: {train_accuracy:.4f}\")\n",
    "    test_loss, test_accuracy = mlp_model.evaluate(X_test_flat, y_test)\n",
    "    print(f\"Hidden Layer Size: {size} | Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1380ad-eb2c-4cb4-af9d-4dceae538ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ce6be89-6e6b-4478-a829-79e4ad41102a",
   "metadata": {},
   "source": [
    "## 5. Based on question 3 and 4, explain the key findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb3a8bdd-da42-47a1-b358-bf4fdfca8ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "layers_performance = {'layers': ['layer_1','layer_2','layer_4','layer_6','layer_8','layer_10',\n",
    "                               'layer_size_50','layer_size_100','layer_size_150','layer_size_200'],\n",
    "    'Training Accuracy':['99.26','99.20','98.83','98.82','98.45','98.30','98.64','98.81','99.07','99.29'],\n",
    "    'Testing Accuracy':['97.76','97.80','97.48','97.42','96.96','97.01','97.22','97.23','97.81','97.84']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e557481a-8142-4167-9277-6d68577a08b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layers</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Testing Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>layer_1</td>\n",
       "      <td>99.26</td>\n",
       "      <td>97.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>layer_2</td>\n",
       "      <td>99.20</td>\n",
       "      <td>97.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>layer_4</td>\n",
       "      <td>98.83</td>\n",
       "      <td>97.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>layer_6</td>\n",
       "      <td>98.82</td>\n",
       "      <td>97.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>layer_8</td>\n",
       "      <td>98.45</td>\n",
       "      <td>96.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>layer_10</td>\n",
       "      <td>98.30</td>\n",
       "      <td>97.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>layer_size_50</td>\n",
       "      <td>98.64</td>\n",
       "      <td>97.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>layer_size_100</td>\n",
       "      <td>98.81</td>\n",
       "      <td>97.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>layer_size_150</td>\n",
       "      <td>99.07</td>\n",
       "      <td>97.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>layer_size_200</td>\n",
       "      <td>99.29</td>\n",
       "      <td>97.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           layers Training Accuracy Testing Accuracy\n",
       "0         layer_1             99.26            97.76\n",
       "1         layer_2             99.20            97.80\n",
       "2         layer_4             98.83            97.48\n",
       "3         layer_6             98.82            97.42\n",
       "4         layer_8             98.45            96.96\n",
       "5        layer_10             98.30            97.01\n",
       "6   layer_size_50             98.64            97.22\n",
       "7  layer_size_100             98.81            97.23\n",
       "8  layer_size_150             99.07            97.81\n",
       "9  layer_size_200             99.29            97.84"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numlayers_layersize = pd.DataFrame(layers_performance)\n",
    "numlayers_layersize"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7520232c-5042-4170-97e0-2894263ac7fb",
   "metadata": {},
   "source": [
    "Varying the Number of Hidden Layers:\n",
    "As you increased the number of hidden layers (from 1 to 2, 4, 6, 8, and 10), you likely observed that the training accuracy improved. Deeper networks can capture more complex features and representations.\n",
    "However, be cautious of overfitting. With too many layers, the model might start memorizing the training data rather than generalizing well to unseen examples (like the test set).\n",
    "The test accuracy might not always increase linearly with the number of layers. There’s a sweet spot where adding more layers improves performance without overfitting.\n",
    "\n",
    "\n",
    "if you look at training accuracy in number of layers it is decreasing whereas if you look at testing accuracy it is somewhere constant like close to 97 to 98% so increasing  number of layers is not beneficial here because model has performed well in 1 and 2 layers."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f64032f-7260-422c-883e-fd176af787fc",
   "metadata": {},
   "source": [
    "varying Hidden Layer Size:\n",
    "Increasing the hidden layer size (neurons per layer) can also improve accuracy initially. Larger layers allow the model to learn more intricate patterns.\n",
    "However, very large hidden layers can lead to longer training times and potential overfitting.\n",
    "Smaller hidden layers might not capture enough complexity, resulting in underfitting.\n",
    "\n",
    "well we are increasing layer size model train accuracy and test accuracy slightly increasing together but it does not have much difference so ideally 100 size is good here our model is not looking over fit because however size increasing test accuracy also increasing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d62084-4b4d-41ca-aae3-1b0826c7b5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9251356-78ec-4202-a505-c55dd22e52b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1062109-1ebc-4b2a-85af-8a07506d6e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe7d5d-d761-4c47-bfa4-4013ca781b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f65e083-de6c-4f3c-adb6-34a390915ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
